{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [System Design Interviews](https://blog.algomaster.io/p/how-to-answer-a-system-design-interview-problem)\n",
    "1. Requirements clarifications\n",
    "   1. Functional requirement\n",
    "   2. Non-Functional requirement\n",
    "   3. Extended Requirement\n",
    "2. Estimation and Constraints\n",
    "3. Data model design\n",
    "4. API design\n",
    "5. High level component design\n",
    "6. Detailed design\n",
    "7. Indetify and resolve bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency and response time\n",
    "1. Response time: The response time is what the client sees: besides the actual time to process the request (the service time), it includes network delays and queueing delays.\n",
    "2. Latency: Latency is the duration that a request is waiting to be handled—during which it is latent, awaiting service.\n",
    "3. Throughput: Throughput refers to the amount of data that can be processed or transmitted in a given amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability:\n",
    "Scalability means having strategies for keeping performance good, even when load increases(ex- more user, adding new features).\n",
    " ## How can we make a system scaleable?\n",
    " 1. Vertical Scaling (Scale up):\n",
    " 2. Horizontal Scaling (Scale out):\n",
    " 3. Load Balancing:\n",
    " 4. Caching\n",
    " 5. Content Delivery Networks (CDNs)\n",
    " 6. Partitioning\n",
    " 7. Asynchronous communication\n",
    " 8. Microservices Architecture\n",
    " 9.  Auto Scalling\n",
    " 10. Multi-region Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4#Caching:\n",
    "A cache's primary purpose is to increase data retrieval performance by reducing the need to access the underlying slower storage layer.\n",
    "\n",
    "***Types of cache:***\n",
    "\n",
    "1. $L1$:\n",
    "   - Closest to CPU core or inside the cpu\n",
    "   - Small in size, usally ranging from `16kb to 128kb`.\n",
    "   - Stores the most frequently accessed data and instructions for immediate use by the CPU.\n",
    "2. L2:\n",
    "   - Inside the CPU\n",
    "   -  Larger than L1, typically ranging from 256 KB to 1 MB per core.\n",
    "3. L3:\n",
    "   - Inside the CPU.\n",
    "   - typically ranging from 2 MB to 50 MB or more.\n",
    "4. L4:\n",
    "   - Outside of the CPU.\n",
    "   - Larger in size.\n",
    "\n",
    "5. Memory Cache: \n",
    "   - involve system RAM or other software-based caching solutions, not the CPU caches.\n",
    "   - `File System Cache:` Part of the operating system that caches frequently accessed files in RAM.\n",
    "   - `Database Cache:` Stores frequently accessed data in RAM to speed up database queries.\n",
    "   - `Application-Level Cache:` Such as Redis or Memcached, which stores frequently used data in RAM to improve application performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache hit and Cache miss\n",
    "1. Cache Hit:\n",
    "    - the data is found and read, it's considered a cache hit.\n",
    "    - hot cache is a instance when data retreeved from `L1`.\n",
    "    - Cool cache is a instance when data retrieved from `L3 or lower`.\n",
    "2. Cache Miss: A cache miss refers to the instance when the memory is searched, and the data isn't found. When this happens, the content is transferred and written into the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write data into cache:\n",
    "##### Cache aside: \n",
    "Data is written into the cache and the corresponding database simultaneously.\n",
    "##### Write through:\n",
    "The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:\n",
    "##### Write behind:\n",
    "Where the write is only done to the caching layer and the write is confirmed as soon as the write to the cache completes. The cache then asynchronously syncs this write to the database.\n",
    "\n",
    "`pros:`<br>\n",
    "1. reduce latency and high throughput for write-intensive applications.\n",
    "\n",
    "`cons:`<br>\n",
    "1. a risk of data loss in case the caching layer crashes.\n",
    "##### Refresh a head:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distrributed cache:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global cache:\n",
    "When the requested data is not found in the global cache, it's the responsibility of the cache to find out the missing piece of data from the underlying data store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDN:\n",
    "A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content. The site's DNS resolution will tell clients which server to contact.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. Users receive content from data centers close to them\n",
    "2. Your servers do not have to serve requests that the CDN fulfills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CDN Types:\n",
    "\n",
    "1. Push CDNs: Push CDNs receive new content whenever changes occur on the server. We take full responsibility for providing content, uploading directly to the CDN, and rewriting URLs to point to the CDN. We can configure when content expires and when it is updated. Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage. <br>Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs. Content is placed on the CDNs once, instead of being re-pulled at regular intervals.\n",
    "\n",
    "2. Pull CDNs: In a Pull CDN situation, the cache is updated based on request. When the client sends a request that requires static assets to be fetched from the CDN if the CDN doesn't have it, then it will fetch the newly updated assets from the origin server and populate its cache with this new asset, and then send this new cached asset to the user. <br> Contrary to the Push CDN, this requires less maintenance because cache updates on CDN nodes are performed based on requests from the client to the origin server. Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.\n",
    "\n",
    "Disadvantages\n",
    "1. As we all know good things come with extra costs, so let's discuss some disadvantages of CDNs:\n",
    "\n",
    "2. Extra charges: It can be expensive to use a CDN, especially for high-traffic services.\n",
    "3. Restrictions: Some organizations and countries have blocked the domains or IP addresses of popular CDNs.\n",
    "4. Location: If most of our audience is located in a country where the CDN has no servers, the data on our website may have to travel further than without using any CDN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Availability\n",
    "Availability is the time a system remains operational to perform its required function in a specific period. It is a simple measure of the percentage of time that a system, service, or machine remains operational under normal conditions.\n",
    "\n",
    "Strategies for Improving Availability\n",
    "1. Redundancy: Redundancy involves having backup components that can take over when primary components fail.\n",
    "   - Server Redundancy: Deploying multiple servers to handle requests, ensuring that if one server fails, others can continue to provide service.\n",
    "   - Database Redundancy: Creating a replica database that can take over if the primary database fails.\n",
    "   - Geographic Redundancy: Distributing resources across multiple geographic locations to mitigate the impact of regional failures.\n",
    "2. Load Balancing: Load balancing distributes incoming network traffic across multiple servers to ensure no single server becomes a bottleneck, enhancing both performance and availability.\n",
    "   - Hardware Load Balancers: Physical devices that distribute traffic based on pre-configured rules.\n",
    "   - Software Load Balancers: Software solutions that manage traffic distribution, such as HAProxy, Nginx, or cloud-based solutions like AWS Elastic Load Balancer.\n",
    "3. Data Replication: Data replication involves copying data from one location to another to ensure that data is available even if one location fails.\n",
    "   - Synchronous Replication: Data is replicated in real-time to ensure consistency across locations.\n",
    "   - Asynchronous Replication: Data is replicated with a delay, which can be more efficient but may result in slight data inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nine's of availability\n",
    "\n",
    "Availability is often quantified by uptime (or downtime) as a percentage of time the service is available. It is generally measured in the number of 9s.\n",
    "\n",
    "$$\n",
    "\\mathcal{Availability = \\frac{Uptime}{(Uptime + Downtime)}}\n",
    "$$\n",
    "\n",
    "If availability is 99.00% available, it is said to have \"2 nines\" of availability, and if it is 99.9%, it is called \"3 nines\", and so on.\n",
    "\n",
    "| Availability (Percent)   | Downtime (Year)    | Downtime (Month)  | Downtime (Week)    |\n",
    "| ------------------------ | ------------------ | ----------------- | ------------------ |\n",
    "| 90% (one nine)           | 36.53 days         | 72 hours          | 16.8 hours         |\n",
    "| 99% (two nines)          | 3.65 days          | 7.20 hours        | 1.68 hours         |\n",
    "| 99.9% (three nines)      | 8.77 hours         | 43.8 minutes      | 10.1 minutes       |\n",
    "| 99.99% (four nines)      | 52.6 minutes       | 4.32 minutes      | 1.01 minutes       |\n",
    "| 99.999% (five nines)     | 5.25 minutes       | 25.9 seconds      | 6.05 seconds       |\n",
    "| 99.9999% (six nines)     | 31.56 seconds      | 2.59 seconds      | 604.8 milliseconds |\n",
    "| 99.99999% (seven nines)  | 3.15 seconds       | 263 milliseconds  | 60.5 milliseconds  |\n",
    "| 99.999999% (eight nines) | 315.6 milliseconds | 26.3 milliseconds | 6 milliseconds     |\n",
    "| 99.9999999% (nine nines) | 31.6 milliseconds  | 2.6 milliseconds  | 0.6 milliseconds   |\n",
    "\n",
    "### Availability in Sequence vs Parallel\n",
    "\n",
    "If a service consists of multiple components prone to failure, the service's overall availability depends on whether the components are in sequence or in parallel.\n",
    "\n",
    "#### Sequence\n",
    "\n",
    "Overall availability decreases when two components are in sequence.\n",
    "\n",
    "$$\n",
    "Availability \\space (Total) = Availability \\space (Foo) * Availability \\space (Bar)\n",
    "$$\n",
    "\n",
    "For example, if both `Foo` and `Bar` each had 99.9% availability, their total availability in sequence would be 99.8%.\n",
    "\n",
    "#### Parallel\n",
    "\n",
    "Overall availability increases when two components are in parallel.\n",
    "\n",
    "$$\n",
    "Availability \\space (Total) = 1 - (1 - Availability \\space (Foo)) * (1 - Availability \\space (Bar))\n",
    "$$\n",
    "\n",
    "For example, if both `Foo` and `Bar` each had 99.9% availability, their total availability in parallel would be 99.9999%.\n",
    "\n",
    "### Availability vs Reliability\n",
    "\n",
    "If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. In other words, high reliability contributes to high availability, but it is possible to achieve high availability even with an unreliable system.\n",
    "\n",
    "### High availability vs Fault Tolerance\n",
    "\n",
    "Both high availability and fault tolerance apply to methods for providing high uptime levels. However, they accomplish the objective differently.\n",
    "\n",
    "A fault-tolerant system has no service interruption but a significantly higher cost, while a highly available system has minimal service interruption. Fault-tolerance requires full hardware redundancy as if the main system fails, with no loss in uptime, another system should take over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability:\n",
    "The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or soft‐ ware faults, and even human error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can We Improve databae performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data partitioning:\n",
    "Data partitioning is a technique to break up a database into many smaller parts. It is the process of splitting up a database or a table across multiple machines to improve the manageability, performance, and availability of a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horizontal Partitioning (or Sharding)\n",
    "In this strategy, we split the table data horizontally based on the range of values defined by the partition key. It is also referred to as database sharding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vertical partitioning:\n",
    "In vertical partitioning, we partition the data vertically based on columns. We divide tables into relatively smaller tables with few elements, and each part is present in a separate partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication\n",
    "Replication is a process that involves sharing information to ensure consistency between redundant resources such as multiple databases, to improve reliability, fault-tolerance, or accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Master-slave replication:\n",
    "The master serves reads and writes, replicating writes to one or more slaves, which serve only reads. Slaves can also replicate to additional slaves in a tree-like fashion. If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.\n",
    "##### Pros:\n",
    "1. Backups of the entire database of relatively no impact on the master.\n",
    "2. Applications can read from the slave(s) without impacting the master.\n",
    "3. Slaves can be taken offline and synced back to the master without any downtime.\n",
    "##### Cons:\n",
    "1. Replication adds more hardware and additional complexity.\n",
    "2. Downtime and possibly loss of data when a master fails.\n",
    "3. All writes also have to be made to the master in a master-slave architecture.\n",
    "4. The more read slaves, the more we have to replicate, which will increase replication lag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Master-master replication\n",
    "Both masters serve reads/writes and coordinate with each other. If either master goes down, the system can continue to operate with both reads and writes.\n",
    "##### Pros:\n",
    "1. Applications can read from both masters.\n",
    "2. Distributes write load across both master nodes.\n",
    "3. Simple, automatic, and quick failover.\n",
    "##### Cons:\n",
    "1. Not as simple as master-slave to configure and deploy.\n",
    "2. Either loosely consistent or have increased write latency due to synchronization.\n",
    "3. Conflict resolution comes into play as more write nodes are added and as latency increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronous replication\n",
    "In synchronous replication, data is written to primary storage and the replica simultaneously. As such, the primary copy and the replica should always remain synchronized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous replication\n",
    "asynchronous replication copies the data to the replica after the data is already written to the primary storage. Although the replication process may occur in near-real-time, it is more common for replication to occur on a scheduled basis and it is more cost-effective. It will violates the `Consistcy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sharding\n",
    "Database sharding is a `horizontal scaling` technique used to split a large database into smaller, independent pieces called shards.\n",
    "\n",
    "Partitioning criteria:\n",
    "1. Hash-Based\n",
    "2. List-Based\n",
    "3. Range Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "1. Availability: Provides logical independence to the partitioned database, ensuring the high availability of our application. Here individual partitions can be managed independently.\n",
    "2. Scalability: Proves to increase scalability by distributing the data across multiple partitions.\n",
    "3. Security: Helps improve the system's security by storing sensitive and non-sensitive data in different partitions. This could provide better manageability and security to sensitive data.\n",
    "4. Query Performance: Improves the performance of the system. Instead of querying the whole database, now the system has to query only a smaller partition.\n",
    "5. Data Manageability: Divides tables and indexes into smaller and more manageable units.\n",
    "6. Geographical Distribution: Sharding allows you to strategically place shards closer to your users, reducing latency and improving the user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cons:\n",
    "1. Complexity: Sharding introduces additional complexity, requiring careful planning and management.\n",
    "\n",
    "2. Data Consistency: Maintaining data consistency across shards can be challenging, especially when data needs to be joined or aggregated from multiple shards.\n",
    "\n",
    "3. Cross-shard Joins: Performing joins across multiple shards can be complex and computationally expensive.\n",
    "\n",
    "4. Data Rebalancing: As data volumes grow, shards may become unevenly distributed, requiring rebalancing to maintain optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denormalization:\n",
    "Denormalization attempts to improve read performance at the expense of some write performance. Redundant copies of the data are written in multiple tables to avoid expensive joins. Some RDBMS such as PostgreSQL and Oracle support materialized views which handle the work of storing redundant information and keeping redundant copies consistent.\n",
    "\n",
    "Pros:\n",
    "1. Retrieving data is faster.\n",
    "2. Writing queries is easier.\n",
    "3. Reduction in number of tables.\n",
    "4. Convenient to manage.\n",
    "\n",
    "Disadvantages\n",
    "1. Expensive inserts and updates.\n",
    "2. Increases complexity of database design.\n",
    "3. Increases data redundancy.\n",
    "4. More chances of data inconsistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACID and BASE consitency Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACID\n",
    "1. Atomicity: Atomicity ensures that a transaction is treated as an indivisible unit of work. It means that either all the operations within a transaction are successfully completed, or none of them are applied to the database.\n",
    "2. Consistency: Consistency ensures that a transaction brings the database from one valid state to another valid state. It means that all the data integrity constraints, such as unique constraints, foreign key constraints, and check constraints, are satisfied before and after the transaction.\n",
    "3. Isolation: Isolation ensures that transactions are executed in a way that they do not interfere with each other. It means that the intermediate state of a transaction is not visible to other transactions until it is committed.\n",
    "4. Durability: Once the transaction has been completed and the writes and updates have been written to the disk, it will remain in the system even if a system failure occurs.\n",
    "\n",
    "While ACID properties are foundational to RDBMS, NoSQL databases like MongoDB often sacrifice some ACID properties for performance and scalability.\n",
    "\n",
    "#### How does it work?\n",
    "1. Logging: Detailed records of all transactions are kept, allowing for recovery in case of a failure.\n",
    "\n",
    "2. Locking: Data is locked during a transaction to prevent concurrent access and ensure isolation.\n",
    "\n",
    "3. Two-Phase Commit: A protocol used to coordinate the commitment of a transaction across multiple systems.\n",
    "\n",
    "### BASE\n",
    "BASE stands for basically available, soft state, and eventually consistent. The acronym highlights that BASE is opposite of ACID, like their chemical equivalents.\n",
    "1. Basically available: Basically available is the database’s concurrent accessibility by users at all times. For example, during a sudden surge in traffic on an ecommerce platform, the system may prioritize serving product listings and accepting orders. Even if there is a slight delay in updating inventory quantities, users continue to check out items.\n",
    "2. Soft Sate: Indicates that the state of the system may change over time, the system may not be in a consistent state at all times.\n",
    "3. Eventually consistent: Eventually consistent means the record will achieve consistency when all the concurrent updates have been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP theorem:\n",
    "CAP theorem states that a distributed system can deliver only two of the three desired characteristics Consistency, Availability, and Partition tolerance (CAP).\n",
    "\n",
    "1. Consistency: Consistency means that all clients see the same data at the same time, no matter which node they connect to. For this to happen, whenever data is written to one node, it must be instantly forwarded or replicated across all the nodes in the system before the write is deemed \"successful\".\n",
    "\n",
    "2. Availability: Availability means that any client making a request for data gets a response, even if one or more nodes are down.\n",
    "3. Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CAP Trade-Off: Choosing 2 out of 3:\n",
    "1. CP (Consistency and Partition Tolerance): `Banking systems`\n",
    "   1. RDBMS: `MySQL and PostgreSQL`\n",
    "2. AP (Availability and Partition Tolerance): A shopping cart system is designed to always accept items, prioritizing availability.\n",
    "   1. NoSQL databases\n",
    "3. CA (Consistency and Availability): In the absence of partitions, a system can be both consistent and available. However, network partitions are inevitable in distributed systems, making this combination impractical. `Single-node databases`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Transactions\n",
    "A distributed transaction is a set of operations on data that is performed across two or more databases. It is typically coordinated across separate nodes connected by a network, but may also span multiple databases on a single server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Phase commit\n",
    "The two-phase commit (2PC) protocol is a distributed algorithm that coordinates all the processes that participate in a distributed transaction on whether to commit or abort (roll back) the transaction.\n",
    "\n",
    "This protocol achieves its goal even in many cases of temporary system failure and is thus widely used. However, it is not resilient to all possible failure configurations, and in rare cases, manual intervention is needed to remedy an outcome.\n",
    "\n",
    "This protocol requires a coordinator node, which basically coordinates and oversees the transaction across different nodes. The coordinator tries to establish the consensus among a set of processes in two phases, hence the name.\n",
    "\n",
    "***Prepare phase:***\n",
    "The prepare phase involves the coordinator node collecting consensus from each of the participant nodes. The transaction will be aborted unless each of the nodes responds that they're prepared.\n",
    "\n",
    "***Commit phase:***\n",
    "If all participants respond to the coordinator that they are prepared, then the coordinator asks all the nodes to commit the transaction. If a failure occurs, the transaction will be rolled back.\n",
    "\n",
    "#### Cons:\n",
    "- What if one of the nodes crashes?\n",
    "- What if the coordinator itself crashes?\n",
    "- It is a blocking protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three-phase commit:\n",
    "Three-phase commit (3PC) is an extension of the two-phase commit where the commit phase is split into two phases. This helps with the blocking problem that occurs in the two-phase commit protocol.\n",
    "\n",
    "1. Prepare phase: This phase is the same as the two-phase commit.\n",
    "\n",
    "2. Pre-commit phase: Coordinator issues the pre-commit message and all the participating nodes must acknowledge it. If a participant fails to receive this message in time, then the transaction is aborted.\n",
    "\n",
    "3. Commit phase: This step is also similar to the two-phase commit protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does Pre-commit phase helpful?\n",
    "1. If the participant nodes are found in this phase, that means that every participant has completed the first phase. The completion of prepare phase is guaranteed.\n",
    "2. Every phase can now time out and avoid indefinite waits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saga\n",
    "A saga is a sequence of local transactions. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails because it violates a business rule then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choreography: Each local transaction publishes domain events that trigger local transactions in other services.\n",
    "2. Orchestration: An orchestrator tells the participants what local transactions to execute.\n",
    "\n",
    "Problems:\n",
    "1. The Saga pattern is particularly hard to debug.\n",
    "2. There's a risk of cyclic dependency between saga participants.\n",
    "3. Lack of participant data isolation imposes durability challenges.\n",
    "4. Testing is difficult because all services must be running to simulate a transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
